{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Implementing Sigmoid Function:**"
      ],
      "metadata": {
        "id": "I2UBX2B9zYsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Task To Do:**\n",
        "\n",
        "• Implement the Logistic Function by completing the code or writing your own function.\n",
        "\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "kYvzXNFlzhNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implementation of Sigmoid Function:*"
      ],
      "metadata": {
        "id": "dmmAvtiZ7ZfH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6CdNKXy3zRj7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def logistic_function(x):\n",
        "  \"\"\"\n",
        "  Computes the logistic function applied to any value of x.\n",
        "  Arguments:\n",
        "  x: scalar or numpy array of any size.\n",
        "  Returns:\n",
        "  y: logistic function applied to x.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  y = 1 / (1 + np.exp(-x))\n",
        "  return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Test Case for logistic function:*"
      ],
      "metadata": {
        "id": "rAZ6JB8oz8Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def test_logistic_function():\n",
        "  \"\"\"\n",
        "  Test cases for the logistic_function.\n",
        "  \"\"\"\n",
        "  # Test with scalar input\n",
        "  x_scalar = 0\n",
        "  expected_output_scalar = round(1 / (1 + np.exp(0)), 3) # Expected output: 0.5\n",
        "  assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
        "  # Test with positive scalar input\n",
        "  x_pos = 2\n",
        "  expected_output_pos = round(1 / (1 + np.exp(-2)), 3) # Expected output: ~0.881\n",
        "  assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
        "  # Test with negative scalar input\n",
        "  x_neg = -3\n",
        "  expected_output_neg = round(1 / (1 + np.exp(3)), 3) # Expected output: ~0.047\n",
        "  assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
        "    # Test with numpy array input\n",
        "  x_array = np.array([0, 2, -3])\n",
        "  expected_output_array = np.array([0.5, 0.881, 0.047]) # Adjusted expected values rounded to 3 decimals\n",
        "  # Use np.round to round the array element-wise and compare\n",
        "  assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
        "  print(\"All tests passed!\")\n",
        "# Run the test case\n",
        "test_logistic_function()"
      ],
      "metadata": {
        "id": "_81UXkeo4m5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31738ed-5215-4129-f03d-3ce49c3ecd97"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Implementing Log Loss Function:**"
      ],
      "metadata": {
        "id": "vxtwUG1I7GFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Task To Do:**\n",
        "\n",
        "• Implement the Log - loss Function by completing the code or writing your own function.\n",
        "\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "Y50N13uH7Rpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implementation of log - loss function:*"
      ],
      "metadata": {
        "id": "Vhu1CYlx7kep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Computes log loss for true target value y ={0 or 1} and predicted target value y’ inbetween {0-1}.\n",
        "  Arguments:\n",
        "  y_true (scalar): true target value {0 or 1}.\n",
        "  y_pred (scalar): predicted taget value {0-1}.\n",
        "  Returns:\n",
        "  loss (float): loss/error value\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  # Ensure y_pred is clipped to avoid log(0)\n",
        "  y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "  loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "oTaabxn57UKG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Verifying the Intution:*"
      ],
      "metadata": {
        "id": "R7qaE6Fy8lqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function:\n",
        "y_true, y_pred = 0, 0.1\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
        "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
        "y_true, y_pred = 1, 0.9\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')"
      ],
      "metadata": {
        "id": "x2WqLf738gxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba003a70-0222-4d73-c08e-f732489ad6c6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log loss(0, 0.1) ==> 0.10536051565782628\n",
            "+++++++++++++--------------------------++++++++++++++++++++++++\n",
            "log loss(1, 0.9) ==> 0.10536051565782628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Test Case for log - loss function:*"
      ],
      "metadata": {
        "id": "5qi5K_2g8ACQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_log_loss():\n",
        "  \"\"\"\n",
        "  Test cases for the log_loss function.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  # Test case 1: Perfect prediction (y_true = 1, y_pred = 1)\n",
        "  y_true = 1\n",
        "  y_pred = 1\n",
        "  expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=1, y_pred=1)\"\n",
        "  # Test case 2: Perfect prediction (y_true = 0, y_pred = 0)\n",
        "  y_true = 0\n",
        "  y_pred = 0\n",
        "  expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction y_true=0, y_pred=0)\"\n",
        "  # Test case 3: Incorrect prediction (y_true = 1, y_pred = 0)\n",
        "  y_true = 1\n",
        "  y_pred = 0\n",
        "  try:\n",
        "    log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "  except ValueError:\n",
        "    pass # Test passed if ValueError is raised for log(0)\n",
        "  # Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)\n",
        "  y_true = 0\n",
        "  y_pred = 1\n",
        "  try:\n",
        "    log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "  except ValueError:\n",
        "    pass # Test passed if ValueError is raised for log(0)\n",
        "  # Test case 5: Partially correct prediction\n",
        "  y_true = 1\n",
        "  y_pred = 0.8\n",
        "  expected_loss = -(1 * np.log(0.8)) - (0 * np.log(0.2)) # ~0.2231\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=1, y_pred=0.8)\"\n",
        "  y_true = 0\n",
        "  y_pred = 0.2\n",
        "  expected_loss = -(0 * np.log(0.2)) - (1 * np.log(0.8)) # ~0.2231\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=0, y_pred=0.2)\"\n",
        "  print(\"All tests passed!\")\n",
        "# Run the test case\n",
        "test_log_loss()"
      ],
      "metadata": {
        "id": "QPX-Gmua8Erc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a171af-e3ce-418d-bc6d-7b24c815f81a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Implementing Cost Function:**"
      ],
      "metadata": {
        "id": "UhtCj7kL8pqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Task To Do:**\n",
        "\n",
        "• Implement the Cost Function by completing the code or writing your own function.\n",
        "\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "aUQoNwm283eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implemenetation of Cost Function:*"
      ],
      "metadata": {
        "id": "otmUQ0-Q8-k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "  Args:\n",
        "  y_true (array_like, shape (n,)): array of true values (0 or 1)\n",
        "  y_pred (array_like, shape (n,)): array of predicted values (probability of y_pred being 1)\n",
        "  Returns:\n",
        "  cost (float): nonnegative cost corresponding to y_true and y_pred\n",
        "  \"\"\"\n",
        "  assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "  n = len(y_true)\n",
        "  loss_vec = log_loss(y_true, y_pred)\n",
        "  cost = np.sum(loss_vec) / n\n",
        "\n",
        "  return cost\n"
      ],
      "metadata": {
        "id": "-seAuyqy82I6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Testing the Cost Function:*"
      ],
      "metadata": {
        "id": "0VeN95PW9KCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def test_cost_function():\n",
        "  # Test case 1: Simple example with known expected cost\n",
        "  y_true = np.array([1, 0, 1])\n",
        "  y_pred = np.array([0.9, 0.1, 0.8])\n",
        "  # Expected output: Manually calculate cost for these values\n",
        "  # log_loss(y_true, y_pred) for each example\n",
        "  expected_cost = (-(1 * np.log(0.9)) - (1 - 1) * np.log(1 - 0.9) +\n",
        "  -(0 * np.log(0.1)) - (1 - 0) * np.log(1 - 0.1) +\n",
        "  -(1 * np.log(0.8)) - (1 - 1) * np.log(1 - 0.8)) / 3\n",
        "\n",
        "  # Call the cost_function to get the result\n",
        "  result = cost_function(y_true, y_pred)\n",
        "  # Assert that the result is close to the expected cost with a tolerance of 1e-6\n",
        "  assert np.isclose(result, expected_cost, atol=1e-6), f\"Test failed: {result} != {expected_cost}\"\n",
        "  print(\"Test passed for simple case!\")\n",
        "# Run the test case\n",
        "test_cost_function()"
      ],
      "metadata": {
        "id": "P7JLTH1n9T9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aecb0611-96f5-4407-e700-710a80d0e3bd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed for simple case!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Extending the cost function for sigmoid regression to be used with model parameters.**"
      ],
      "metadata": {
        "id": "3bUvU1wx40QC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Task To Do:**\n",
        "\n",
        "• Implement the vectorized cost function costfunction logreg Function by completing the\n",
        "code or writing your own function.\n",
        "\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "9yf3v4wU43YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implementation of Cost Function for Logistic/Sigmoid Regression:*"
      ],
      "metadata": {
        "id": "eEOH7oCc5D2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute cost function in terms of model parameters - using vectorization\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "  \"\"\"\n",
        "  Computes the cost function, given data and model parameters.\n",
        "  Args:\n",
        "  X (ndarray, shape (m,n)): data on features, m observations with n features.\n",
        "  y (array_like, shape (m,)): array of true values of target (0 or 1).\n",
        "  w (array_like, shape (n,)): weight parameters of the model.\n",
        "  b (float): bias parameter of the model.\n",
        "  Returns:\n",
        "  cost (float): nonnegative cost corresponding to y and y_pred.\n",
        "  \"\"\"\n",
        "  n, d = X.shape\n",
        "  assert len(y) == n, \"Number of feature observations and number of target observations do not match.\"\n",
        "  assert len(w) == d, \"Number of features and number of weight parameters do not match.\"\n",
        "  # Compute z using np.dot\n",
        "  z = np.dot(X, w) + b # Matrix-vector multiplication and adding bias\n",
        "  # Compute predictions using logistic function (sigmoid)\n",
        "  y_pred = logistic_function(z)\n",
        "  # Compute the cost using the cost function\n",
        "  cost = cost_function(y, y_pred)\n",
        "  return cost\n",
        "# Testing the Function:\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMEOa0A15IHO",
        "outputId": "0b6ca3da-61ea-41ce-eb80-a3d59a26c8ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost for logistic regression(X = [[ 10  20]\n",
            " [-10  10]], y = [1 0], w = [0.5 1.5], b = 1) = 5.500008350834906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Implementing Gradient Descent for Training Sigmoid Regression:**"
      ],
      "metadata": {
        "id": "tgs53TUu5hwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Task To Do:**\n",
        "\n",
        "• Implement the compute gradient Function by completing the code or writing your own\n",
        "function.\n",
        "\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "L_52Q6Or5wyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Computing Gradients for Sigmoid Regression:*"
      ],
      "metadata": {
        "id": "AeftB3Mc50Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "  \"\"\"\n",
        "  Computes gradients of the cost function with respect to model parameters.\n",
        "  Args:\n",
        "  X (ndarray, shape (n,d)): Input data, n observations with d features\n",
        "  y (array_like, shape (n,)): True labels (0 or 1)\n",
        "  w (array_like, shape (d,)): Weight parameters of the model\n",
        "  b (float): Bias parameter of the model\n",
        "  Returns:\n",
        "  grad_w (array_like, shape (d,)): Gradients of the cost function with respect to the weight\n",
        "  parameters\n",
        "  grad_b (float): Gradient of the cost function with respect to the bias parameter\n",
        "  \"\"\"\n",
        "  n, d = X.shape # X has shape (n, d)\n",
        "  assert len(y) == n, f\"Expected y to have {n} elements, but got {len(y)}\"\n",
        "  assert len(w) == d, f\"Expected w to have {d} elements, but got {len(w)}\"\n",
        "  z = np.dot(X, w) + b\n",
        "  # Compute predictions using logistic function (sigmoid)\n",
        "  y_pred = logistic_function(z) # Compute z = X * w + b\n",
        "  error = y_pred - y # Compute error\n",
        "  # Compute gradients\n",
        "  grad_w = - (1 / n) * np.dot(X.T, error) # Gradient w.r.t weights, shape (d,)\n",
        "  grad_b = - (1 / n) * np.sum(error) # Gradient w.r.t bias, scalar\n",
        "  return grad_w, grad_b"
      ],
      "metadata": {
        "id": "QRUO3hAW53ef"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*A simple assertion test for compute gradient function:*"
      ],
      "metadata": {
        "id": "-gRA4flK59_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple test case\n",
        "X = np.array([[10, 20], [-10, 10]]) # shape (2, 2)\n",
        "y = np.array([1, 0]) # shape (2,)\n",
        "w = np.array([0.5, 1.5]) # shape (2,)\n",
        "b = 1 # scalar\n",
        "# Assertion tests\n",
        "try:\n",
        "  grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "  print(\"Gradients computed successfully.\")\n",
        "  print(f\"grad_w: {grad_w}\")\n",
        "  print(f\"grad_b: {grad_b}\")\n",
        "except AssertionError as e:\n",
        "  print(f\"Assertion error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti27V-bwBBp9",
        "outputId": "f75eefd6-aec9-4560-ac12-919fdd89dc06"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients computed successfully.\n",
            "grad_w: [ 4.99991649 -4.99991649]\n",
            "grad_b: -0.4999916492890759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2 Gradient Descent for Sigmoid Regression:**"
      ],
      "metadata": {
        "id": "Kc-osgs-6Tjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Task To Do:**\n",
        "\n",
        "• Implement the gradient descent Function by completing the code or writing your own\n",
        "function.\n",
        "\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "ARoeYd986fxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Gradient Descent for Sigmoid Regression:*"
      ],
      "metadata": {
        "id": "u72mTPYC6bmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "  \"\"\"\n",
        "  Implements batch gradient descent to optimize logistic regression parameters.\n",
        "  Args:\n",
        "  X (ndarray, shape (n,d)): Data on features, n observations with d features\n",
        "  y (array_like, shape (n,)): True values of target (0 or 1)\n",
        "  w (array_like, shape (d,)): Initial weight parameters\n",
        "  b (float): Initial bias parameter\n",
        "  alpha (float): Learning rate\n",
        "  n_iter (int): Number of iterations\n",
        "  show_cost (bool): If True, displays cost every 100 iterations\n",
        "  show_params (bool): If True, displays parameters every 100 iterations\n",
        "  Returns:\n",
        "  w (array_like, shape (d,)): Optimized weight parameters\n",
        "  b (float): Optimized bias parameter\n",
        "  cost_history (list): List of cost values over iterations\n",
        "  params_history (list): List of parameters (w, b) over iterations\n",
        "  \"\"\"\n",
        "  n, d = X.shape\n",
        "  assert len(y) == n, \"Number of observations in X and y do not match\"\n",
        "  assert len(w) == d, \"Number of features in X and w do not match\"\n",
        "  cost_history = []\n",
        "  params_history = []\n",
        "  for i in range(n_iter):\n",
        "    # Compute gradients\n",
        "    grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "    # Update weights and bias\n",
        "    w -= alpha * grad_w\n",
        "    b -= alpha * grad_b\n",
        "    # Compute cost\n",
        "    cost = costfunction_logreg(X, y, w, b)\n",
        "    # Store cost and parameters\n",
        "    cost_history.append(cost)\n",
        "    params_history.append((w.copy(), b))\n",
        "    # Optionally print cost and parameters\n",
        "    if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "      print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "    if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
        "      print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
        "  return w, b, cost_history, params_history\n",
        "# Test the gradient_descent function with sample data\n",
        "X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "y = np.array([1, 0]) # Shape (2,)\n",
        "w = np.zeros(X.shape[1]) # Shape (2,) - same as number of features\n",
        "b = 0.0 # Scalar\n",
        "alpha = 0.01 # Learning rate\n",
        "n_iter = 10000 # Number of iterations\n",
        "# Perform gradient descent\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=True, show_params=False)\n",
        "# Print final parameters and cost\n",
        "print(\"\\nFinal parameters:\")\n",
        "print(f\"w: {w_out}, b: {b_out}\")\n",
        "print(f\"Final cost: {cost_history[-1]:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WirPhp46lfR",
        "outputId": "85dd0b98-dd00-4c4b-d65f-49feced37b2f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Cost = 0.693178\n",
            "\n",
            "Final parameters:\n",
            "w: [-0.0005  -0.00025], b: 0.0\n",
            "Final cost: 0.693178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*A simple assertion test for gradient descent Function:*"
      ],
      "metadata": {
        "id": "70xTq4jf7fIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gradient_descent():\n",
        "  X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "  y = np.array([1, 0]) # Shape (2,)\n",
        "  w = np.zeros(X.shape[1]) # Shape (2,)\n",
        "  b = 0.0 # Scalar\n",
        "  alpha = 0.0001 # Learning rate, further reduced\n",
        "  n_iter = 1000 # Number of iterations, increased\n",
        "  # Run gradient descent\n",
        "  w_out, b_out, cost_history, _ = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=False)\n",
        "  # Assertions\n",
        "  assert len(cost_history) == n_iter, \"Cost history length does not match the number of iterations\"\n",
        "  assert w_out.shape == w.shape, \"Shape of output weights does not match the initial weights\"\n",
        "  assert isinstance(b_out, float), \"Bias output is not a float\"\n",
        "  assert cost_history[-1] < cost_history[0], \"Cost did not decrease over iterations\"\n",
        "  print(\"All tests passed!\")\n",
        "# Run the test\n",
        "test_gradient_descent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "SOjpIcka7jyI",
        "outputId": "d82cfa10-28a2-43bd-eeb2-6ed28ec86c94"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Cost did not decrease over iterations",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3331216224.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Run the test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3331216224.py\u001b[0m in \u001b[0;36mtest_gradient_descent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mw_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape of output weights does not match the initial weights\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bias output is not a float\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cost did not decrease over iterations\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Run the test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Cost did not decrease over iterations"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Convergence of Cost During Gradient Descent:**"
      ],
      "metadata": {
        "id": "uOPs3uqG7znj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code for Plotting Cost Vs. Iteration during Gradient Descent:*"
      ],
      "metadata": {
        "id": "IneWDlU7741N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plotting cost over iteration\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel(\"Iteration\", fontsize = 14)\n",
        "plt.ylabel(\"Cost\", fontsize = 14)\n",
        "plt.title(\"Cost vs Iteration\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "collapsed": true,
        "id": "NPYO0_VU777D",
        "outputId": "d3848d97-daa9-4f29-8b63-118b2d2920cf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Decision/Prediction Function for Binary Classification:**"
      ],
      "metadata": {
        "id": "u3s1_onz8C_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Task To Do:**\n",
        "\n",
        "• Implement the prediction Function by completing the code or writing your own function.\n",
        "\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "9vQbJoJw8HRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Decision/Prediction Function:*"
      ],
      "metadata": {
        "id": "R2SUQcAW8Mdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def prediction(X, w, b, threshold=0.5):\n",
        "  \"\"\"\n",
        "  Predicts binary outcomes for given input features based on logistic regression parameters.\n",
        "  Arguments:\n",
        "  X (ndarray, shape (n,d)): Array of test independent variables (features) with n samples and d\n",
        "  features.\n",
        "  w (ndarray, shape (d,)): Array of weights learned via gradient descent.\n",
        "  b (float): Bias learned via gradient descent.\n",
        "  threshold (float, optional): Classification threshold for predicting class labels. Default is 0.5.\n",
        "  Returns:\n",
        "  y_pred (ndarray, shape (n,)): Array of predicted dependent variable (binary class labels: 0 or 1).\n",
        "  \"\"\"\n",
        "  z = np.dot(X, w) + b\n",
        "  # Compute the predicted probabilities using the logistic function\n",
        "  y_test_prob = logistic_function(z) # z = wx + b\n",
        "  # Classify based on the threshold\n",
        "  y_pred = (y_test_prob >= threshold).astype(int)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "DrQ4f7r98P4v"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*A simple assertion test for Prediction Function:*"
      ],
      "metadata": {
        "id": "G1WZcnQo8Xp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prediction():\n",
        "  X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]]) # Shape (3, 2)\n",
        "  w_test = np.array([1.0, -1.0]) # Shape (2,)\n",
        "  b_test = 0.0 # Scalar bias\n",
        "  threshold = 0.5 # Default threshold\n",
        "  # Updated expected output\n",
        "  expected_output = np.array([0, 1, 1])\n",
        "  # Call the prediction function\n",
        "  y_pred = prediction(X_test, w_test, b_test, threshold)\n",
        "  # Assert that the output matches the expected output\n",
        "  assert np.array_equal(y_pred, expected_output), f\"Expected {expected_output}, but got {y_pred}\"\n",
        "  print(\"Test passed!\")\n",
        "test_prediction()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITMWzu2R8aSa",
        "outputId": "33d71027-3a00-44a9-de49-6393340b4a53"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. Evaluating Classifier:**"
      ],
      "metadata": {
        "id": "chU412oJ8q94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Task To Do:**\n",
        "\n",
        "• Implement evaluate classification Function by completing the code or writing your own\n",
        "function.\n",
        "\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "cWCkzugY8zZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Evaluation of the Classifier:*"
      ],
      "metadata": {
        "id": "vY0dsorM830Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Computes the confusion matrix, precision, recall, and F1-score for binary classification.\n",
        "  Arguments:\n",
        "  y_true (ndarray, shape (n,)): Ground truth binary labels (0 or 1).\n",
        "  y_pred (ndarray, shape (n,)): Predicted binary labels (0 or 1).\n",
        "  Returns:\n",
        "  metrics (dict): A dictionary containing confusion matrix, precision, recall, and F1-score.\n",
        "  \"\"\"\n",
        "  # Initialize confusion matrix components\n",
        "  TP = np.sum((y_true == 1) & (y_pred == 1)) # True Positives\n",
        "  TN = np.sum((y_true == 0) & (y_pred == 0)) # True Negatives\n",
        "  FP = np.sum((y_true == 0) & (y_pred == 1)) # False Positives\n",
        "  FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "  # Confusion matrix\n",
        "  confusion_matrix = np.array([[TN, FP],[FN, TP]])\n",
        "  # Precision, recall, and F1-score\n",
        "  precision = TP / (TP + FP) if (TP + FP) > 0.0 else 0.0\n",
        "  recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0.0 else 0.0\n",
        "  # Metrics dictionary\n",
        "  metrics = {\n",
        "    \"confusion_matrix\": confusion_matrix,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1_score\": f1_score\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "ies_I7Fc86yi"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}